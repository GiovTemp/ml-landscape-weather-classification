{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the path to the folder containing the training images\n",
    "train_dir = '../../datasets/dataset1/train/'\n",
    "\n",
    "# Define the classes and their corresponding numerical labels\n",
    "class_labels = ['cloudy', 'rain', 'shine', 'sunrise']\n",
    "\n",
    "# Create a dictionary that maps each label to a numerical value\n",
    "label_dict = {k: v for v, k in enumerate(class_labels)}\n",
    "\n",
    "# Read the images from the training folder, resize them to 64x64 pixels, and flatten them into feature vectors\n",
    "train_images = []\n",
    "train_labels = []\n",
    "for c in class_labels:\n",
    "    class_dir = os.path.join(train_dir, c)\n",
    "    for file in os.listdir(class_dir):\n",
    "        img = cv2.imread(os.path.join(class_dir, file))\n",
    "        img_resized = cv2.resize(img, (128, 128))\n",
    "\n",
    "        # Applica la soglia di Otsu all'immagine in scala di grigi\n",
    "        gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        train_images.append(gray.flatten())\n",
    "        train_labels.append(label_dict[c])\n",
    "X_train = np.array(train_images)\n",
    "y_train = np.array(train_labels)\n",
    "\n",
    "# Split the dataset into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the range of hyperparameters to search over\n",
    "param_grid = {'n_neighbors': [3, 5, 7 , 9 , 11]}\n",
    "\n",
    "# Perform a grid search with 5-fold cross-validation to find the best hyperparameters\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and train a KNN classifier with them\n",
    "best_params = grid_search.best_params_\n",
    "clf = KNeighborsClassifier(**best_params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance of the model on the validation set using several metrics\n",
    "y_pred = clf.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "precision = precision_score(y_val, y_pred, average='weighted')\n",
    "recall = recall_score(y_val, y_pred, average='weighted')\n",
    "f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "# Print the metrics and the best hyperparameters\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "# Visualize the confusion matrix, predicted labels distribution, and true labels distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(y_pred, color='blue')\n",
    "plt.title('Predicted Labels Distribution')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(y_val, color='green')\n",
    "plt.title('True Labels Distribution')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leggi le immagini di test e crea le matrici di feature e target\n",
    "test_dir = '../../datasets/dataset1/test/'\n",
    "test_images = []\n",
    "test_labels = []\n",
    "for c in class_labels:\n",
    "    class_dir = os.path.join(test_dir, c)\n",
    "    for file in os.listdir(class_dir):\n",
    "        img = cv2.imread(os.path.join(class_dir, file))\n",
    "        img_resized = cv2.resize(img, (128, 128))\n",
    "\n",
    "        # Applica il filtro di edge detection per rilevare i contorni\n",
    "        img_filtered = cv2.Canny(img_filtered, 100, 200)\n",
    "\n",
    "        img_normalized = img_filtered / 255.0 # normalizza i pixel tra 0 e 1\n",
    "        \n",
    "        test_images.append(img_normalized.flatten())\n",
    "        test_labels.append(label_dict[c])\n",
    "X_test = np.array(test_images)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "\n",
    "# Valuta le prestazioni del modello sul dataset di test\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calcola le metriche di valutazione del modello\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Stampa le metriche\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Valuta il modello su ogni classe utilizzando la confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "# Visualizza la confusion matrix come un grafico\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()\n",
    "\n",
    "# Visualizza la distribuzione delle etichette predette dal modello sui dati di validation\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(y_pred, color='blue')\n",
    "plt.title('Predicted Labels Distribution')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Visualizza la distribuzione delle etichette reali sui dati di validation\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(y_test, color='green')\n",
    "plt.title('True Labels Distribution')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
